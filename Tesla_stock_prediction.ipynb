{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenaAbbasova/Tesla_stock_prediction/blob/main/Tesla_stock_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Problem Statement and Agenda:\n",
        "\n",
        "One of the applications of Time Series Forecasting is to predict opening stock prices, closing stock prices, and the volume of stocks to be traded, among others. In this dataset, we aim to forecast the future behavior of the stock market, focusing on the average stock price data of Tesla spanning from 2010 to 2022.\n",
        "\n",
        "For this case study, we will divide our data into training and test sets, build our models on the training data, forecast for the test data timestamps, and then evaluate using the Root Mean Squared Error (RMSE) model evaluation metric.\n",
        "\n",
        "The following topics will be covered in this case study:\n",
        "\n",
        "Exploratory Data Analysis\n",
        "ARIMA/SARIMA models (with and without exogenous variables)\n",
        "Facebook Prophet Model\n",
        "LSTM Model (Deep Learning Model)\n"
      ],
      "metadata": {
        "id": "kcKbeuWiCsft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "SGPS9-sUM7Dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59IGCV_C_Hm_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "from pandas import Series\n",
        "from numpy import log\n",
        "import plotly.express as px # high level interface\n",
        "import plotly.graph_objects as go # lower level interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for advanced modeling\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "6ee2fathUbc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the data\n",
        "data=pd.read_csv('/content/TSLA.csv')"
      ],
      "metadata": {
        "id": "G6Ay2cjO_OwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "VgB2EShr_agv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze moving averages to capture trends\n",
        "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
        "data['MA200'] = data['Close'].rolling(window=200).mean()\n",
        "data[['Close', 'MA50', 'MA200']].plot(figsize=(14, 7), title=\"Tesla Closing Price with Moving Averages\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qlgFVj4cTPHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=data.copy()"
      ],
      "metadata": {
        "id": "8ftlkA8aVs8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.set_index('Date',inplace=True)"
      ],
      "metadata": {
        "id": "9zQ6ngvUVzlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1.head()"
      ],
      "metadata": {
        "id": "hqtAlLBrV-kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()"
      ],
      "metadata": {
        "id": "WbRxIbdG_bT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "GD50Kb-BUCmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "MmWr5XgdNFzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZmUfwV5ANmZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Rgeaz5WINyJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Typical Price\n",
        "df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
        "\n",
        "# Calculate TPV (Typical Price * Volume)\n",
        "df['TPV'] = df['Typical_Price'] * df['Volume']\n",
        "\n",
        "# Calculate Cumulative TPV and Cumulative Volume\n",
        "df['Cumulative_TPV'] = df['TPV'].cumsum()\n",
        "df['Cumulative_Volume'] = df['Volume'].cumsum()\n",
        "\n",
        "# Calculate VWAP\n",
        "df['VWAP'] = df['Cumulative_TPV'] / df['Cumulative_Volume']\n",
        "\n"
      ],
      "metadata": {
        "id": "nllq79Ro8yyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Kn1X5Zjp9B1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we going to keep only 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP' variables\n",
        "\n",
        "# Drop intermediate columns if not needed\n",
        "df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']]"
      ],
      "metadata": {
        "id": "ZsgkCrmgN7HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ZLW-IP-QObtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DURBIN - WATSON TEST"
      ],
      "metadata": {
        "id": "AMbi_jptPw2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Durbin-Watson test going to be applied to residuals from time series models to diagnose autocorrelation."
      ],
      "metadata": {
        "id": "H0XaLDkuOdJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "print(sm.stats.durbin_watson(df['VWAP']))\n",
        "print(sm.stats.durbin_watson(df['Open']))\n",
        "print(sm.stats.durbin_watson(df['High']))\n",
        "print(sm.stats.durbin_watson(df['Low']))\n",
        "print(sm.stats.durbin_watson(df['Close']))\n",
        "print(sm.stats.durbin_watson(df['Volume']))"
      ],
      "metadata": {
        "id": "1R3XCh92P5ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The presence of strong positive autocorrelation in the residuals of the 'VWAP', 'Open', 'High', 'Low',\n",
        "# and 'Close' variables implies that their current values are highly dependent on their past values."
      ],
      "metadata": {
        "id": "Rc6yykrcQe1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "OIgfB44JR-up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "KELTTBjxSCGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = df['Date'].min()\n",
        "end_date = df['Date'].max()\n",
        "print(start_date)\n",
        "print(end_date)"
      ],
      "metadata": {
        "id": "qzWnD2UHSUEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "cfSp9rkDSrsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check frequency of VWAP variable\n",
        "df.VWAP.plot(figsize=(20,5),title='VWAP')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1ZDebSy5Ug2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'date' to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "E74VRPz_KpbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df ['year'] = df['Date'].dt.year\n",
        "df['month'] = df ['Date'].dt.month\n"
      ],
      "metadata": {
        "id": "ODp3daOcKWtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "xZftHtKhLl-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.boxplot(x=df['year'], y =df['VWAP'],palette='pastel')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "LWvzRMPOTJ7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can observe from the graph how the volume-weighted average price has been increasing since 2010,\n",
        "# with the best year for selling being 2021."
      ],
      "metadata": {
        "id": "7IBlQehoVWrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.boxplot(x=df['month'], y =df['VWAP'],palette='pastel')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "BAjtJN-mXOMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selling in November was good, as the prices were high.\n"
      ],
      "metadata": {
        "id": "eAd8JfBNXVu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n"
      ],
      "metadata": {
        "id": "vIMkXPsSZDAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decompose the time series into trend, seasonal, cyclical, and irregular components.\n",
        "decompose = seasonal_decompose(df['VWAP'], model='additive', period=365)\n",
        "\n",
        "# Extract the components\n",
        "trend = decompose.trend\n",
        "seasonal = decompose.seasonal\n",
        "cyclical = df['VWAP']  - trend - seasonal\n",
        "irregular = decompose.resid"
      ],
      "metadata": {
        "id": "sscCJCwvVHDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the components\n",
        "plt.subplot(411)\n",
        "plt.plot(df['VWAP'], label='Original')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(412)\n",
        "plt.plot(trend, label='Trend')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(413)\n",
        "plt.plot(seasonal, label='Seasonality')\n",
        "plt.legend(loc='upper left')\n",
        "plt.subplot(414)\n",
        "plt.plot(irregular, label='Residuals')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z_FZU7f_U2DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Q plot"
      ],
      "metadata": {
        "id": "QsnbnNCLaeGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let´s see whether a dataset follow a normal distribution"
      ],
      "metadata": {
        "id": "qxVhyTOJDseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats\n",
        "import pylab"
      ],
      "metadata": {
        "id": "9-fGPF2QZC9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scipy.stats.probplot(df.VWAP, plot=pylab)\n",
        "plt.title(\"QQ plot for Volume Waighted Price Average\")\n",
        "pylab.show()"
      ],
      "metadata": {
        "id": "eSZOZuuQZC6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Replace 'desired_distribution' with the name of the distribution you want to test against (e.g., 'expon', 'gamma', 'uniform', etc.)\n",
        "stats.probplot(df['VWAP'], dist='expon', plot=plt)\n",
        "plt.title(\"QQ plot for Volume Weighted Price Average\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z3P4xCPZZC00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trend=decompose.trend\n",
        "seasonality=decompose.seasonal\n",
        "residual=decompose.resid\n",
        "\n",
        "print('Trend', '\\n', trend.head(12), '\\n')\n",
        "\n",
        "print('Seasonality', '\\n', seasonality.head(12), '\\n')\n",
        "\n",
        "print('Residual', '\\n', residual.head(12), '\\n')"
      ],
      "metadata": {
        "id": "e-Dq-zxDZCun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new column\n",
        "df['Time_Stamp']=pd.DataFrame(df, columns=['Date'])\n",
        "df['Time_Stamp'] = pd.to_datetime(df['Time_Stamp'])\n",
        "# set index column 'Time_Stamp'\n",
        "df_final = df.set_index('Time_Stamp')\n"
      ],
      "metadata": {
        "id": "N7RrDufwZChp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "_okRONjff4Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spit the data into training and test before building Time Series Forecasting"
      ],
      "metadata": {
        "id": "1UaiReL3kqsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.index.max()"
      ],
      "metadata": {
        "id": "cbg3IkE8dwcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.index.min()"
      ],
      "metadata": {
        "id": "NDBOCc_OdwY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into training and tesing data\n",
        "train_df = df_final[pd.to_datetime(df_final['Date']) < pd.to_datetime('2017-10-04')]\n",
        "\n",
        "test_df = df_final[pd.to_datetime(df_final['Date']) >= pd.to_datetime('2017-10-04')]"
      ],
      "metadata": {
        "id": "9Mqi7VehdwUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "EUZ7bEtwdwQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape"
      ],
      "metadata": {
        "id": "vAv0RoF8dwLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = train_df[['VWAP']]\n",
        "test_final = test_df[['VWAP']]"
      ],
      "metadata": {
        "id": "XuKKDK1tdwEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_final.head()"
      ],
      "metadata": {
        "id": "SPq4zUiLdwA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_final.head()"
      ],
      "metadata": {
        "id": "G07mKinsn5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmented Dickey Fuller Test"
      ],
      "metadata": {
        "id": "NxjlkXrgpGgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To check time series stationary or not\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller"
      ],
      "metadata": {
        "id": "_nlT9TMln8b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adfuller(train_final['VWAP'])"
      ],
      "metadata": {
        "id": "4kcD5lQvpfNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_adftest(timeseries):\n",
        "  result=adfuller(timeseries)\n",
        "  print('Augmented Dickey Fuller Test')\n",
        "  labels = ['ADF test', 'P-value','#Lags', 'No of observation']\n",
        "\n",
        "  for i, j in zip(result,labels):\n",
        "    print(j + '------->'+str(i))\n",
        "\n",
        "  if result[1] <= 0.05:\n",
        "    print('Strong evidence against Null Hypothesis and my time series is Stationary')\n",
        "\n",
        "  else:\n",
        "    print('Weak Evidence against Null Hypothesis and my time series is Non-Stationary')"
      ],
      "metadata": {
        "id": "j9bzbPCRpxwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_adftest(train_final['VWAP'])"
      ],
      "metadata": {
        "id": "CPLt2fyXqP7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as time series is non - stationary\n",
        "# we aply Differencing technique"
      ],
      "metadata": {
        "id": "DshYR5tdrYa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference the time series and plot\n",
        "diff_data = train_final['VWAP'].diff().dropna()\n",
        "plt.plot(diff_data)\n",
        "plt.title('Differenced Time Series')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "si7uU8t2qURW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_adftest(diff_data)"
      ],
      "metadata": {
        "id": "Y1x9noZXsMdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code generates combinations of parameters for a SARIMA (Seasonal Autoregressive Integrated Moving Average) model.\n",
        "# SARIMA models are extensions of the ARIMA model that include seasonal components."
      ],
      "metadata": {
        "id": "YU4adnigtuMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "p = q = range(0,3)\n",
        "\n",
        "d = range(0,1)\n",
        "\n",
        "pdq = list(itertools.product(p, d, q)) # Trend\n",
        "\n",
        "model_pdq = [(x[0], x[1], x[2], 5) for x in list(itertools.product(p,d,q))] # Seasonality\n",
        "\n",
        "print('Example of combination for model...........')\n",
        "print('Model : {}{}'.format(pdq[1],model_pdq[1]))\n",
        "print('Model : {}{}'.format(pdq[0],model_pdq[0]))\n",
        "print('Model : {}{}'.format(pdq[2],model_pdq[2]))\n",
        "print('Model : {}{}'.format(pdq[1],model_pdq[2]))"
      ],
      "metadata": {
        "id": "xwYPf2A6sTZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build SARIMAX model - Seasonality Autoregressive integrated moving average with external factor"
      ],
      "metadata": {
        "id": "ti8ADajHuSTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SARIMAX is a statistical model used for time series forecasting. It's an extension of the ARIMA model that incorporates additional features to handle seasonality and exogenous variables (variables external to the model that can influence the time series being analyzed)."
      ],
      "metadata": {
        "id": "694i8c0UF21R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex_train =  train_df\n",
        "ex_test = test_df"
      ],
      "metadata": {
        "id": "M_lFlKvFuR4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_train.head()"
      ],
      "metadata": {
        "id": "HZtwHszFwi9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_obj=pd.DataFrame(columns=['param','seasonal','AIC'])\n",
        "df_obj"
      ],
      "metadata": {
        "id": "7N0hRnoNwnIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure train_final and ex_train are numeric and aligned\n",
        "train_final = train_final.select_dtypes(include=['float64', 'int64'])\n",
        "ex_train = ex_train.select_dtypes(include=['float64', 'int64'])\n",
        "ex_train = ex_train.reindex(train_final.index)\n",
        "\n",
        "# Initialize an empty list to store results\n",
        "results = []\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in model_pdq:\n",
        "        try:\n",
        "            model = sm.tsa.statespace.SARIMAX(train_final, exog=ex_train, order=param, seasonal_order=param_seasonal,\n",
        "                                              enforce_stationarity=False, enforce_invertibility=False)\n",
        "            result_SARIMAX = model.fit()\n",
        "            print('SARIMAX{}{} - AIC:{}'.format(param, param_seasonal, result_SARIMAX.aic))\n",
        "\n",
        "            # Append the results to the list\n",
        "            results.append({'param': param, 'seasonal': param_seasonal, 'AIC': result_SARIMAX.aic})\n",
        "        except Exception as e:\n",
        "            print(f\"Error with SARIMAX{param}{param_seasonal}: {e}\")\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df_obj = pd.DataFrame(results)\n",
        "print(df_obj)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DTIZ9ZJOaQjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_obj.head()"
      ],
      "metadata": {
        "id": "0zrWyZY3ZRT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = df_obj.loc[df_obj['AIC'].idxmin()]\n",
        "best_model"
      ],
      "metadata": {
        "id": "nsaXQvNGY_hF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(0, 0, 0)\t(0, 0, 0, 5)\t\t-38738.804187\n",
        "model = sm.tsa.statespace.SARIMAX(train_final, exog=ex_train, order=(0, 0, 0),\n",
        "                                   seasonal_order=(0, 0, 0, 5),\n",
        "                                   enforce_stationarity=False, enforce_invertibility=False)\n",
        "result = model.fit()\n",
        "print(result.summary())\n"
      ],
      "metadata": {
        "id": "EUfyIsutKioD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The SARIMAX model successfully completed, demonstrating a strong fit to the VWAP data with a nearly perfect coefficient for VWAP (1.0000, 𝑝 < 0.001). However, other predictors (Open, High, Low, Close, Volume) showed no significant impact, suggesting they may not contribute meaningfully to the model's performance. The model encountered some warnings regarding singular covariance and convergence issues, which may indicate potential instability in the parameter estimates. Diagnostic tests (Ljung-Box, Jarque-Bera) highlight residual autocorrelation and non-normality. Further refinement, including addressing heteroskedasticity and ensuring parameter stability, could enhance model reliability."
      ],
      "metadata": {
        "id": "mCDYnvKnOalh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.plot_diagnostics(figsize=(16,8))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8FZoE9KKIuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns used during training\n",
        "exog_train_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']\n",
        "\n",
        "# Filter ex_test to match the training exogenous variables\n",
        "ex_test = ex_test[exog_train_columns]\n",
        "\n",
        "# Ensure no missing values and proper numeric types\n",
        "ex_test = ex_test.fillna(0).astype(float)\n",
        "\n",
        "# Perform forecasting\n",
        "predict_SARIMAX = result.get_forecast(steps=len(test_df), exog=ex_test)\n",
        "predicted_mean = predict_SARIMAX.predicted_mean\n",
        "print(predicted_mean)\n"
      ],
      "metadata": {
        "id": "9beHZiMwjGdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import *\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "vOZOhwIbIx3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = sqrt(mean_squared_error(test_final.VWAP, predict_SARIMAX.predicted_mean, squared=False))\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "9OGHJ12GI0BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The RMSE (Root Mean Square Error) value of 3.637539544312077e-05 indicates an extremely low error, suggesting that the SARIMAX model is highly accurate in its predictions."
      ],
      "metadata": {
        "id": "C7-1DevtS49Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_final, label='Training Data')\n",
        "plt.plot(test_final, label='Test Data')\n",
        "plt.plot(test_final.index, predict_SARIMAX.predicted_mean, label='predicted Model -SARIMAX')\n",
        "plt.legend(loc='best')\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "V6iIFDOgI5jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8y_yF7maTDqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sarima Model"
      ],
      "metadata": {
        "id": "-kyriISYI9_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.tsa.statespace.SARIMAX(train_final['VWAP'],order=(1,1,2),\n",
        "                                   seasonal_order=(1,0,2,5),\n",
        "                                   enforce_stationarity=False, enforce_invertibility=False)\n",
        "result = model.fit()\n",
        "print(result.summary())"
      ],
      "metadata": {
        "id": "QpOI90zyI_JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_SARIMA = result.get_forecast(steps=len(test_df), exog=ex_test)\n",
        "predict_SARIMA.predicted_mean"
      ],
      "metadata": {
        "id": "3zp3Q3c8JRC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = sqrt(mean_squared_error(test_final.VWAP, predict_SARIMA.predicted_mean, squared=False))\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "5ACNcZUvJQ_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_final, label='Training Data')\n",
        "plt.plot(test_final, label='Test Data')\n",
        "plt.plot(test_final.index, predict_SARIMAX.predicted_mean, label='predicted Model -SARIMAX')\n",
        "plt.plot(test_final.index, predict_SARIMA.predicted_mean, label='predicted Model -SARIMA')\n",
        "plt.legend(loc='best')\n",
        "plt.grid();"
      ],
      "metadata": {
        "id": "LBE7Km22JQ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The SARIMAX model performs close to the actual values and provides satisfactory forecast accuracy, it suggests that incorporating exogenous variables has helped capture important information in our time series data."
      ],
      "metadata": {
        "id": "4aJYWhcrTVxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "d9HXy2awf9PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "qRwnWDYLURdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### LSTM Model Implementation\n",
        "# Scale data for LSTM\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "data_scaled = scaler.fit_transform(df[['VWAP']])"
      ],
      "metadata": {
        "id": "JvSoX9-6UKJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test\n",
        "train_size = int(len(df_final) * 0.8)\n",
        "train, test = df_final[:train_size], df_final[train_size:]"
      ],
      "metadata": {
        "id": "BVmgeFpbU6me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for LSTM\n",
        "train_scaled = data_scaled[:train_size]\n",
        "test_scaled = data_scaled[train_size - 50:]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7ctft9R5JQ5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences for LSTM\n",
        "def create_sequences(data, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        sequences.append(data[i:i + sequence_length])\n",
        "        targets.append(data[i + sequence_length])\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "sequence_length = 50\n",
        "train_sequences, train_targets = create_sequences(train_scaled, sequence_length)\n",
        "test_sequences, test_targets = create_sequences(test_scaled, sequence_length)"
      ],
      "metadata": {
        "id": "Ey8jG_jGVVsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for LSTM\n",
        "train_sequences = train_sequences.reshape(train_sequences.shape[0], train_sequences.shape[1], 1)\n",
        "test_sequences = test_sequences.reshape(test_sequences.shape[0], test_sequences.shape[1], 1)"
      ],
      "metadata": {
        "id": "7NnTljS2Vhk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dense(25),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "tY3m0h9_VlJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "rj_JLoW9VoRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "lstm_model.fit(train_sequences, train_targets, epochs=10, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "id": "8rMMhPASVrfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "lstm_predictions = lstm_model.predict(test_sequences)"
      ],
      "metadata": {
        "id": "zEi7mWYHVvH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reverse scaling\n",
        "lstm_predictions = scaler.inverse_transform(lstm_predictions)\n",
        "test_targets = scaler.inverse_transform(test_targets.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "QJohxMv4Vy6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions vs actual values\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(test_targets, label='Actual Values', color='blue')\n",
        "plt.plot(lstm_predictions, label='LSTM Predictions', color='orange')\n",
        "plt.title('LSTM Predictions vs Actual Values')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PZ5UoiqFV1ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RMSE\n",
        "lstm_rmse = sqrt(mean_squared_error(test_targets, lstm_predictions))\n",
        "print(\"LSTM RMSE:\", lstm_rmse)"
      ],
      "metadata": {
        "id": "zDoZJxYtV4Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In this project, we focused on predicting the VWAP using 12 year's worth of time series data. Throughout the project, we followed a systematic approach to build and evaluate time series models.\n",
        "\n",
        "We began by reading and preprocessing the data, ensuring that it was in a suitable format for analysis. We then conducted exploratory data analysis (EDA) to gain insights into the patterns, trends, and seasonality present in the data.\n",
        "\n",
        "To proceed with modeling, we first checked the stationarity of the time series. Since the data was non - stationary, we performed differencing technique.\n",
        "\n",
        "We applied Durbin-Watson test to residuals from time series models to diagnose autocorrelation.\n",
        "\n",
        "We built  models like Sarima, Sarimax and LSTM model. The performance of these models was evaluated based on both the p-value and root mean squared error (RMSE).\n",
        "\n",
        "After evaluating the models, we found that Sarimax model outperformed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K7QAyPReZs2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2h4EG2Q2rm97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}